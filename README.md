# Qwen Code

<div align="center">

![Qwen Code Screenshot](./docs/assets/qwen-screenshot.png)

[![npm version](https://img.shields.io/npm/v/@qwen-code/qwen-code.svg)](https://www.npmjs.com/package/@qwen-code/qwen-code)
[![License](https://img.shields.io/github/license/QwenLM/qwen-code.svg)](./LICENSE)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D20.0.0-brightgreen.svg)](https://nodejs.org/)
[![Downloads](https://img.shields.io/npm/dm/@qwen-code/qwen-code.svg)](https://www.npmjs.com/package/@qwen-code/qwen-code)

**AI-powered command-line workflow tool for developers**

[Installation](#installation) ‚Ä¢ [Quick Start](#quick-start) ‚Ä¢ [Features](#key-features) ‚Ä¢ [Documentation](./docs/) ‚Ä¢ [Contributing](./CONTRIBUTING.md)

</div>

<div align="center">
  
  <a href="https://qwenlm.github.io/qwen-code-docs/de/">Deutsch</a> | 
  <a href="https://qwenlm.github.io/qwen-code-docs/fr">fran√ßais</a> | 
  <a href="https://qwenlm.github.io/qwen-code-docs/ja/">Êó•Êú¨Ë™û</a> | 
  <a href="https://qwenlm.github.io/qwen-code-docs/ru">–†—É—Å—Å–∫–∏–π</a> | 
  <a href="https://qwenlm.github.io/qwen-code-docs/zh/">‰∏≠Êñá</a>
  
</div>

Qwen Code is a powerful command-line AI workflow tool adapted from [**Gemini CLI**](https://github.com/google-gemini/gemini-cli) ([details](./README.gemini.md)), specifically optimized for [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder) models. It enhances your development workflow with advanced code understanding, automated tasks, and intelligent assistance.

## üí° Free Options Available

Get started with Qwen Code at no cost using any of these free options:

### üî• Qwen OAuth (Recommended)

- **2,000 requests per day** with no token limits
- **60 requests per minute** rate limit
- Simply run `qwen` and authenticate with your qwen.ai account
- Automatic credential management and refresh
- Use `/auth` command to switch to Qwen OAuth if you have initialized with OpenAI compatible mode

### üåè Regional Free Tiers

- **Mainland China**: ModelScope offers **2,000 free API calls per day**
- **International**: OpenRouter provides **up to 1,000 free API calls per day** worldwide

For detailed setup instructions, see [Authorization](#authorization).

> [!WARNING]
> **Token Usage Notice**: Qwen Code may issue multiple API calls per cycle, resulting in higher token usage (similar to Claude Code). We're actively optimizing API efficiency.

## Key Features

- **Code Understanding & Editing** - Query and edit large codebases beyond traditional context window limits
- **Workflow Automation** - Automate operational tasks like handling pull requests and complex rebases
- **Enhanced Parser** - Adapted parser specifically optimized for Qwen-Coder models

## Installation

### Prerequisites

Ensure you have [Node.js version 20](https://nodejs.org/en/download) or higher installed.

```bash
curl -qL https://www.npmjs.com/install.sh | sh
```

### Install from npm

```bash
npm install -g @qwen-code/qwen-code@latest
qwen --version
```

### Install from source

```bash
git clone https://github.com/QwenLM/qwen-code.git
cd qwen-code
npm install
npm install -g .
```

### Install globally with Homebrew (macOS/Linux)

```bash
brew install qwen-code
```

## Quick Start

```bash
# Start Qwen Code
qwen

# Example commands
> Explain this codebase structure
> Help me refactor this function
> Generate unit tests for this module
```

#### Quick Start with Ollama (Local)

```bash
# Ensure Ollama is running and a model is available
ollama serve &            # if not already running
ollama pull qwen2.5-coder

# Configure Qwen Code to use Ollama
export OLLAMA_HOST="http://localhost:11434"
export OLLAMA_MODE="qwen2.5-coder"

# Launch Qwen Code
qwen
```

#### Quick Start with Docker

You can run Qwen Code using Docker in two ways:

##### Option 1: Pull from GitHub Container Registry (Recommended)

1. **Pull the Docker image from GitHub Container Registry:**
```bash
docker pull ghcr.io/legido-ai-workspace/qwen-code-ollama:latest
```

2. **Run with direct API access:**
```bash
docker run -it --rm ghcr.io/legido-ai-workspace/qwen-code-ollama:latest qwen
```

3. **Run with Ollama support (accessing Ollama running on host):**
```bash
# First ensure Ollama is running on the host:
ollama serve &

# Pull a model:
ollama pull qwen2.5-coder

# Run the container with host network access to connect to Ollama:
docker run -it --rm --network host ghcr.io/legido-ai-workspace/qwen-code-ollama:latest qwen --ollama-host http://localhost:11434
```

4. **Run with environment variables:**
```bash
docker run -it --rm -e OPENAI_API_KEY=your_api_key -e OPENAI_BASE_URL=your_api_endpoint ghcr.io/legido-ai-workspace/qwen-code-ollama:latest qwen
```

5. **Mount a project directory:**
```bash
docker run -it --rm -v /path/to/your/project:/workspace ghcr.io/legido-ai-workspace/qwen-code-ollama:latest qwen
```

##### Option 2: Build the Docker Image Locally

If you prefer to build the image locally:

1. **Clone the repository and build the Docker image:**
```bash
git clone https://github.com/legido-ai-workspace/qwen-code-ollama.git
cd qwen-code-ollama
docker build -t qwen-code-ollama .
```

2. **Run using the locally built image:**
```bash
docker run -it --rm qwen-code-ollama qwen
```

> **Note for Docker setup:** When using Ollama with Docker, you'll typically need `--network host` to allow the container to access the Ollama service running on your host machine. Alternatively, you can use `--add-host host.docker.internal:host-gateway` on some Docker versions.

### Docker Image Tags

The Docker image is published with multiple tags:
- `latest` - Most recent build from main branch
- `main` - Build from main branch
- `vX.X.X` - Versioned releases (e.g., `v0.0.12`)

### Session Management

Control your token usage with configurable session limits to optimize costs and performance.

#### Configure Session Token Limit

Create or edit `.qwen/settings.json` in your home directory:

```json
{
  "sessionTokenLimit": 32000
}
```

#### Session Commands

- **`/compress`** - Compress conversation history to continue within token limits
- **`/clear`** - Clear all conversation history and start fresh
- **`/stats`** - Check current token usage and limits

> üìù **Note**: Session token limit applies to a single conversation, not cumulative API calls.

### Authorization

Choose your preferred authentication method based on your needs:

#### 1. Qwen OAuth (üöÄ Recommended - Start in 30 seconds)

The easiest way to get started - completely free with generous quotas:

```bash
# Just run this command and follow the browser authentication
qwen
```

**What happens:**

1. **Instant Setup**: CLI opens your browser automatically
2. **One-Click Login**: Authenticate with your qwen.ai account
3. **Automatic Management**: Credentials cached locally for future use
4. **No Configuration**: Zero setup required - just start coding!

**Free Tier Benefits:**

- ‚úÖ **2,000 requests/day** (no token counting needed)
- ‚úÖ **60 requests/minute** rate limit
- ‚úÖ **Automatic credential refresh**
- ‚úÖ **Zero cost** for individual users
- ‚ÑπÔ∏è **Note**: Model fallback may occur to maintain service quality

#### 2. OpenAI-Compatible API

Use API keys for OpenAI or other compatible providers:

**Configuration Methods:**

1. **Environment Variables**

   ```bash
   export OPENAI_API_KEY="your_api_key_here"
   export OPENAI_BASE_URL="your_api_endpoint"
   export OPENAI_MODEL="your_model_choice"
   ```

2. **Project `.env` File**
   Create a `.env` file in your project root:
   ```env
   OPENAI_API_KEY=your_api_key_here
   OPENAI_BASE_URL=your_api_endpoint
   OPENAI_MODEL=your_model_choice
   ```

**API Provider Options**

> ‚ö†Ô∏è **Regional Notice:**
>
> - **Mainland China**: Use Alibaba Cloud Bailian or ModelScope
> - **International**: Use Alibaba Cloud ModelStudio or OpenRouter

<details>
<summary><b>üá®üá≥ For Users in Mainland China</b></summary>

**Option 1: Alibaba Cloud Bailian** ([Apply for API Key](https://bailian.console.aliyun.com/))

```bash
export OPENAI_API_KEY="your_api_key_here"
export OPENAI_BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
export OPENAI_MODEL="qwen3-coder-plus"
```

**Option 2: ModelScope (Free Tier)** ([Apply for API Key](https://modelscope.cn/docs/model-service/API-Inference/intro))

- ‚úÖ **2,000 free API calls per day**
- ‚ö†Ô∏è Connect your Aliyun account to avoid authentication errors

```bash
export OPENAI_API_KEY="your_api_key_here"
export OPENAI_BASE_URL="https://api-inference.modelscope.cn/v1"
export OPENAI_MODEL="Qwen/Qwen3-Coder-480B-A35B-Instruct"
```

</details>

<details>
<summary><b>üåç For International Users</b></summary>

**Option 1: Alibaba Cloud ModelStudio** ([Apply for API Key](https://modelstudio.console.alibabacloud.com/))

```bash
export OPENAI_API_KEY="your_api_key_here"
export OPENAI_BASE_URL="https://dashscope-intl.aliyuncs.com/compatible-mode/v1"
export OPENAI_MODEL="qwen3-coder-plus"
```

**Option 2: OpenRouter (Free Tier Available)** ([Apply for API Key](https://openrouter.ai/))

```bash
export OPENAI_API_KEY="your_api_key_here"
export OPENAI_BASE_URL="https://openrouter.ai/api/v1"
export OPENAI_MODEL="qwen/qwen3-coder:free"
```

</details>

### üñ•Ô∏è Use Ollama (Local, OpenAI-compatible)

Qwen Code supports local Ollama servers via the OpenAI-compatible API surface. No API key is required.

- Ollama install guide: `https://ollama.com/`
- Pull a model (example):
  ```bash
  ollama pull qwen2.5-coder
  # or: ollama pull qwen2.5-coder:14b
  ```

You can configure Ollama in either of two ways:

1) Using dedicated Ollama environment variables (recommended)
```bash
# Point to your local Ollama server (default port 11434)
export OLLAMA_HOST="http://localhost:11434"
# Choose the model to use from `ollama list`
export OLLAMA_MODE="qwen2.5-coder"

# Run Qwen Code
qwen
```

2) Using OpenAI-compatible variables
```bash
# OpenAI-compatible base URL (note the /v1 suffix)
export OPENAI_BASE_URL="http://localhost:11434/v1"
# OpenAI-compatible model name (e.g., one shown by `ollama list`)
export OPENAI_MODEL="qwen2.5-coder"
# API key is not required by Ollama; a placeholder is fine
export OPENAI_API_KEY="ollama"

# Run Qwen Code
qwen
```

CLI flags alternative (no env vars):
```bash
qwen --openai-base-url http://localhost:11434/v1 -m qwen2.5-coder
```

Notes:
- No real API key is required for Ollama.
- If `OLLAMA_HOST` is set, Qwen Code will auto-select OpenAI-compatible mode.
- Some Ollama models have limited tool/function-calling support; Qwen Code automatically adapts when targeting Ollama.

## Usage Examples

### üîç Explore Codebases

```bash
cd your-project/
qwen

# Architecture analysis
> Describe the main pieces of this system's architecture
> What are the key dependencies and how do they interact?
> Find all API endpoints and their authentication methods
```

### üíª Code Development

```bash
# Refactoring
> Refactor this function to improve readability and performance
> Convert this class to use dependency injection
> Split this large module into smaller, focused components

# Code generation
> Create a REST API endpoint for user management
> Generate unit tests for the authentication module
> Add error handling to all database operations
```

### üîÑ Automate Workflows

```bash
# Git automation
> Analyze git commits from the last 7 days, grouped by feature
> Create a changelog from recent commits
> Find all TODO comments and create GitHub issues

# File operations
> Convert all images in this directory to PNG format
> Rename all test files to follow the *.test.ts pattern
> Find and remove all console.log statements
```

### üêõ Debugging & Analysis

```bash
# Performance analysis
> Identify performance bottlenecks in this React component
> Find all N+1 query problems in the codebase

# Security audit
> Check for potential SQL injection vulnerabilities
> Find all hardcoded credentials or API keys
```

## Popular Tasks

### üìö Understand New Codebases

```text
> What are the core business logic components?
> What security mechanisms are in place?
> How does the data flow through the system?
> What are the main design patterns used?
> Generate a dependency graph for this module
```

### üî® Code Refactoring & Optimization

```text
> What parts of this module can be optimized?
> Help me refactor this class to follow SOLID principles
> Add proper error handling and logging
> Convert callbacks to async/await pattern
> Implement caching for expensive operations
```

### üìù Documentation & Testing

```text
> Generate comprehensive JSDoc comments for all public APIs
> Write unit tests with edge cases for this component
> Create API documentation in OpenAPI format
> Add inline comments explaining complex algorithms
> Generate a README for this module
```

### üöÄ Development Acceleration

```text
> Set up a new Express server with authentication
> Create a React component with TypeScript and tests
> Implement a rate limiter middleware
> Add database migrations for new schema
> Configure CI/CD pipeline for this project
```

## Commands & Shortcuts

### Session Commands

- `/help` - Display available commands
- `/clear` - Clear conversation history
- `/compress` - Compress history to save tokens
- `/stats` - Show current session information
- `/exit` or `/quit` - Exit Qwen Code

### Keyboard Shortcuts

- `Ctrl+C` - Cancel current operation
- `Ctrl+D` - Exit (on empty line)
- `Up/Down` - Navigate command history

## Benchmark Results

### Terminal-Bench Performance

| Agent     | Model              | Accuracy |
| --------- | ------------------ | -------- |
| Qwen Code | Qwen3-Coder-480A35 | 37.5%    |
| Qwen Code | Qwen3-Coder-30BA3B | 31.3%    |

## Development & Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md) to learn how to contribute to the project.

For detailed authentication setup, see the [authentication guide](./docs/cli/authentication.md).

## Troubleshooting

If you encounter issues, check the [troubleshooting guide](docs/troubleshooting.md).

- Ollama requests failing with 401/403: ensure you‚Äôre using `OLLAMA_HOST` or `OPENAI_BASE_URL` pointing to `http://localhost:11434/v1`. No real API key is required; if needed, set `OPENAI_API_KEY=ollama`.

## Acknowledgments

This project is based on [Google Gemini CLI](https://github.com/google-gemini/gemini-cli). We acknowledge and appreciate the excellent work of the Gemini CLI team. Our main contribution focuses on parser-level adaptations to better support Qwen-Coder models.

## License

[LICENSE](./LICENSE)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=QwenLM/qwen-code&type=Date)](https://www.star-history.com/#QwenLM/qwen-code&Date)
